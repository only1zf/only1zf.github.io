"use strict";(globalThis.webpackChunkpub=globalThis.webpackChunkpub||[]).push([[9137],{28453(e,n,t){t.d(n,{R:()=>o,x:()=>a});var s=t(96540);const i={},r=s.createContext(i);function o(e){const n=s.useContext(r);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:o(e.components),s.createElement(r.Provider,{value:n},e.children)}},79289(e,n,t){t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>p,frontMatter:()=>o,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"excerpts/AI/gpt-5.2-prompting-guide","title":"GPT-5.2 Prompting Guide","description":"1. Introduction","source":"@site/docs/excerpts/AI/gpt-5.2-prompting-guide.md","sourceDirName":"excerpts/AI","slug":"/excerpts/AI/gpt-5.2-prompting-guide","permalink":"/docs/excerpts/AI/gpt-5.2-prompting-guide","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"excerptsSidebar","previous":{"title":"\u4e00\u4e2a\u76f8\u5bf9\u901a\u7528\u7684\u7ffb\u8bd1 Prompt\uff0c\u53ef\u4ee5\u9002\u7528\u4e8e\u591a\u79cd\u4e0d\u540c\u7684\u8bed\u8a00\u7ffb\u8bd1","permalink":"/docs/excerpts/AI/a-common-translation-prompt-for-different-languages"},"next":{"title":"GPT \u65e0\u6cd5\u7ffb\u8bd1\u8d85\u957f\u5185\u5bb9\u7684\u63d0\u793a\u8bcd\u4f18\u5316\u5c1d\u8bd5","permalink":"/docs/excerpts/AI/gpt-translation-long-content-optimization"}}');var i=t(74848),r=t(28453);const o={},a="GPT-5.2 Prompting Guide",l={},c=[{value:"1. Introduction",id:"1-introduction",level:2},{value:"2. Key behavioral differences",id:"2-key-behavioral-differences",level:2},{value:"3. Prompting patterns",id:"3-prompting-patterns",level:2},{value:"3.1 Controlling verbosity and output shape",id:"31-controlling-verbosity-and-output-shape",level:3},{value:"3.2 Preventing Scope drift (e.g., UX / design in frontend tasks)",id:"32-preventing-scope-drift-eg-ux--design-in-frontend-tasks",level:3},{value:"3.3 Long-context and recall",id:"33-long-context-and-recall",level:3},{value:"3.4 Handling ambiguity &amp; hallucination risk",id:"34-handling-ambiguity--hallucination-risk",level:3},{value:"4. Compaction (Extending Effective Context)",id:"4-compaction-extending-effective-context",level:2},{value:"5. Agentic steerability &amp; user updates",id:"5-agentic-steerability--user-updates",level:2},{value:"6. Tool-calling and parallelism",id:"6-tool-calling-and-parallelism",level:2},{value:"7. Structured extraction, PDF, and Office workflows",id:"7-structured-extraction-pdf-and-office-workflows",level:2},{value:"8. Prompt Migration Guide to GPT 5.2",id:"8-prompt-migration-guide-to-gpt-52",level:2},{value:"9. Web search and research",id:"9-web-search-and-research",level:2},{value:"10. Conclusion",id:"10-conclusion",level:2},{value:"Appendix",id:"appendix",level:2},{value:"Example prompt for a web research agent:",id:"example-prompt-for-a-web-research-agent",level:3}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,r.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"gpt-52-prompting-guide",children:"GPT-5.2 Prompting Guide"})}),"\n",(0,i.jsx)(n.h2,{id:"1-introduction",children:"1. Introduction"}),"\n",(0,i.jsx)(n.p,{children:"GPT-5.2 is our newest flagship model for enterprise and agentic workloads, designed to deliver higher accuracy, stronger instruction following, and more disciplined execution across complex workflows. Building on GPT-5.1, GPT-5.2 improves token efficiency on medium-to-complex tasks, produces cleaner formatting with less unnecessary verbosity, and shows clear gains in structured reasoning, tool grounding, and multimodal understanding."}),"\n",(0,i.jsx)(n.p,{children:"GPT-5.2 is especially well-suited for production agents that prioritize reliability, evaluability, and consistent behavior. It performs strongly across coding, document analysis, finance, and multi-tool agentic scenarios, often matching or exceeding leading models on task completion. At the same time, it remains prompt-sensitive and highly steerable in tone, verbosity, and output shape, making explicit prompting an important part of successful deployments."}),"\n",(0,i.jsx)(n.p,{children:"While GPT-5.2 works well out of the box for many use cases, this guide focuses on prompt patterns and migration practices that maximize performance in real production systems. These recommendations are drawn from internal testing and customer feedback, where small changes to prompt structure, verbosity constraints, and reasoning settings often translate into large gains in correctness, latency, and developer trust."}),"\n",(0,i.jsx)(n.h2,{id:"2-key-behavioral-differences",children:"2. Key behavioral differences"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Compared with previous generation models (e.g. GPT-5 and GPT-5.1), GPT-5.2 delivers:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"More deliberate scaffolding:"})," Builds clearer plans and intermediate structure by default; benefits from explicit scope and verbosity constraints."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Generally lower verbosity:"})," More concise and task-focused, though still prompt-sensitive and preference needs to be articulated in the prompt."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Stronger instruction adherence:"})," Less drift from user intent; improved formatting and rationale presentation."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Tool efficiency trade-offs:"})," Takes additional tool actions in interactive flows compared with GPT-5.1, can be further optimized via prompting."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Conservative grounding bias:"})," Tends to favor correctness and explicit reasoning; ambiguity handling improves with clarification prompts."]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"This guide focuses on prompting GPT-5.2 to maximize its strengths \u2014 higher intelligence, accuracy, grounding, and discipline \u2014 while mitigating remaining inefficiencies. Existing GPT-5 / GPT-5.1 prompting guidance largely carries over and remains applicable."}),"\n",(0,i.jsx)(n.h2,{id:"3-prompting-patterns",children:"3. Prompting patterns"}),"\n",(0,i.jsx)(n.p,{children:"Adapt following themes into your prompts for better steer on GPT-5.2"}),"\n",(0,i.jsx)(n.h3,{id:"31-controlling-verbosity-and-output-shape",children:"3.1 Controlling verbosity and output shape"}),"\n",(0,i.jsxs)(n.p,{children:["Give ",(0,i.jsx)(n.strong,{children:"clear and concrete length constraints"})," especially in enterprise and coding agents."]}),"\n",(0,i.jsx)(n.p,{children:"Example clamp adjust based on desired verbosity:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"<output_verbosity_spec>\n- Default: 3\u20136 sentences or \u22645 bullets for typical answers.\n- For simple \u201cyes/no + short explanation\u201d questions: \u22642 sentences.\n- For complex multi-step or multi-file tasks:\n  - 1 short overview paragraph\n  - then \u22645 bullets tagged: What changed, Where, Risks, Next steps, Open questions.\n- Provide clear and structured responses that balance informativeness with conciseness. Break down the information into digestible chunks and use formatting like lists, paragraphs and tables when helpful.\n- Avoid long narrative paragraphs; prefer compact bullets and short sections.\n- Do not rephrase the user\u2019s request unless it changes semantics.\n</output_verbosity_spec>\n"})}),"\n",(0,i.jsx)(n.h3,{id:"32-preventing-scope-drift-eg-ux--design-in-frontend-tasks",children:"3.2 Preventing Scope drift (e.g., UX / design in frontend tasks)"}),"\n",(0,i.jsx)(n.p,{children:"GPT-5.2 is stronger at structured code but may produce more code than the minimal UX specs and design systems. To stay within the scope, explicitly forbid extra features and uncontrolled styling."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"<design_and_scope_constraints>\n- Explore any existing design systems and understand it deeply.\n- Implement EXACTLY and ONLY what the user requests.\n- No extra features, no added components, no UX embellishments.\n- Style aligned to the design system at hand.\n- Do NOT invent colors, shadows, tokens, animations, or new UI elements, unless requested or necessary to the requirements.\n- If any instruction is ambiguous, choose the simplest valid interpretation.\n</design_and_scope_constraints>\n"})}),"\n",(0,i.jsx)(n.p,{children:"For design system enforcement, reuse your 5.1 <design_system_enforcement> block but add \u201cno extra features\u201d and \u201ctokens-only colors\u201d for extra emphasis."}),"\n",(0,i.jsx)(n.h3,{id:"33-long-context-and-recall",children:"3.3 Long-context and recall"}),"\n",(0,i.jsxs)(n.p,{children:["For long-context tasks, the prompt may benefit from ",(0,i.jsx)(n.strong,{children:"force summarization and re-grounding"}),". This pattern reduces \u201clost in the scroll\u201d errors and improves recall over dense contexts."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"<long_context_handling>\n- For inputs longer than ~10k tokens (multi-chapter docs, long threads, multiple PDFs):\n  - First, produce a short internal outline of the key sections relevant to the user\u2019s request.\n  - Re-state the user\u2019s constraints explicitly (e.g., jurisdiction, date range, product, team) before answering.\n  - In your answer, anchor claims to sections (\u201cIn the \u2018Data Retention\u2019 section\u2026\u201d) rather than speaking generically.\n- If the answer depends on fine details (dates, thresholds, clauses), quote or paraphrase them.\n</long_context_handling>\n"})}),"\n",(0,i.jsx)(n.h3,{id:"34-handling-ambiguity--hallucination-risk",children:"3.4 Handling ambiguity & hallucination risk"}),"\n",(0,i.jsx)(n.p,{children:"Configure the prompt for overconfident hallucinations on ambiguous queries (e.g., unclear requirements, missing constraints, or questions that need fresh data but no tools are called)."}),"\n",(0,i.jsx)(n.p,{children:"Mitigation prompt:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"<uncertainty_and_ambiguity>\n- If the question is ambiguous or underspecified, explicitly call this out and:\n  - Ask up to 1\u20133 precise clarifying questions, OR\n  - Present 2\u20133 plausible interpretations with clearly labeled assumptions.\n- When external facts may have changed recently (prices, releases, policies) and no tools are available:\n  - Answer in general terms and state that details may have changed.\n- Never fabricate exact figures, line numbers, or external references when you are uncertain.\n- When you are unsure, prefer language like \u201cBased on the provided context\u2026\u201d instead of absolute claims.\n</uncertainty_and_ambiguity>\n"})}),"\n",(0,i.jsx)(n.p,{children:"You can also add a short self-check step for high-risk outputs:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"<high_risk_self_check>\nBefore finalizing an answer in legal, financial, compliance, or safety-sensitive contexts:\n- Briefly re-scan your own answer for:\n  - Unstated assumptions,\n  - Specific numbers or claims not grounded in context,\n  - Overly strong language (\u201calways,\u201d \u201cguaranteed,\u201d etc.).\n- If you find any, soften or qualify them and explicitly state assumptions.\n</high_risk_self_check>\n"})}),"\n",(0,i.jsx)(n.h2,{id:"4-compaction-extending-effective-context",children:"4. Compaction (Extending Effective Context)"}),"\n",(0,i.jsx)(n.p,{children:"For long-running, tool-heavy workflows that exceed the standard context window, GPT-5.2 with Reasoning supports response compaction via the /responses/compact endpoint. Compaction performs a loss-aware compression pass over prior conversation state, returning encrypted, opaque items that preserve task-relevant information while dramatically reducing token footprint. This allows the model to continue reasoning across extended workflows without hitting context limits."}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"When to use compaction"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Multi-step agent flows with many tool calls"}),"\n",(0,i.jsx)(n.li,{children:"Long conversations where earlier turns must be retained"}),"\n",(0,i.jsx)(n.li,{children:"Iterative reasoning beyond the maximum context window"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Key properties"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Produces opaque, encrypted items (internal logic may evolve)"}),"\n",(0,i.jsx)(n.li,{children:"Designed for continuation, not inspection"}),"\n",(0,i.jsx)(n.li,{children:"Compatible with GPT-5.2 and Responses API"}),"\n",(0,i.jsx)(n.li,{children:"Safe to run repeatedly in long sessions"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Compact a Response"})}),"\n",(0,i.jsx)(n.p,{children:"Endpoint"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"POST https://api.openai.com/v1/responses/compact\n"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"What it does"})}),"\n",(0,i.jsx)(n.p,{children:"Runs a compaction pass over a conversation and returns a compacted response object. Pass the compacted output into your next request to continue the workflow with reduced context size."}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Best practices"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Monitor context usage and plan ahead to avoid hitting context window limits"}),"\n",(0,i.jsx)(n.li,{children:"Compact after major milestones (e.g., tool-heavy phases), not every turn"}),"\n",(0,i.jsx)(n.li,{children:"Keep prompts functionally identical when resuming to avoid behavior drift"}),"\n",(0,i.jsx)(n.li,{children:"Treat compacted items as opaque; don\u2019t parse or depend on internals"}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:["For guidance on when and how to compact in production, see the ",(0,i.jsx)(n.a,{href:"https://platform.openai.com/docs/guides/conversation-state?api-mode=responses",children:"Conversation State"})," guide and ",(0,i.jsx)(n.a,{href:"https://platform.openai.com/docs/api-reference/responses/compact",children:"Compact a Response"})," page."]}),"\n",(0,i.jsx)(n.p,{children:"Here is an example:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'from openai import OpenAI\nimport json\n\n\nclient = OpenAI()\n\n\nresponse = client.responses.create(\n   model="gpt-5.2",\n   input=[\n       {\n           "role": "user",\n           "content": "write a very long poem about a dog.",\n       },\n   ]\n)\n\n\noutput_json = [msg.model_dump() for msg in response.output]\n\n\n# Now compact, passing the original user prompt and the assistant text as inputs\ncompacted_response = client.responses.compact(\n   model="gpt-5.2",\n   input=[\n       {\n           "role": "user",\n           "content": "write a very long poem about a dog.",\n       },\n       output_json[0]\n   ]\n)\n\n\nprint(json.dumps(compacted_response.model_dump(), indent=2))\n'})}),"\n",(0,i.jsx)(n.h2,{id:"5-agentic-steerability--user-updates",children:"5. Agentic steerability & user updates"}),"\n",(0,i.jsx)(n.p,{children:"GPT-5.2 is strong on agentic scaffolding and multi-step execution when prompted well. You can reuse your GPT-5.1 <user_updates_spec> and <solution_persistence> blocks."}),"\n",(0,i.jsx)(n.p,{children:"Two key tweaks could be added to further push the performance of GPT-5.2:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Clamp verbosity of updates (shorter, more focused)."}),"\n",(0,i.jsx)(n.li,{children:"Make scope discipline explicit (don\u2019t expand problem surface area)."}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"Example updated spec:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"<user_updates_spec>\n- Send brief updates (1\u20132 sentences) only when:\n  - You start a new major phase of work, or\n  - You discover something that changes the plan.\n- Avoid narrating routine tool calls (\u201creading file\u2026\u201d, \u201crunning tests\u2026\u201d).\n- Each update must include at least one concrete outcome (\u201cFound X\u201d, \u201cConfirmed Y\u201d, \u201cUpdated Z\u201d).\n- Do not expand the task beyond what the user asked; if you notice new work, call it out as optional.\n</user_updates_spec>\n"})}),"\n",(0,i.jsx)(n.h2,{id:"6-tool-calling-and-parallelism",children:"6. Tool-calling and parallelism"}),"\n",(0,i.jsx)(n.p,{children:"GPT-5.2 improves on 5.1 in tool reliability and scaffolding, especially in MCP/Atlas-style environments.\nBest practices as applicable to GPT-5 / 5.1:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Describe tools crisply: 1\u20132 sentences for what they do and when to use them."}),"\n",(0,i.jsx)(n.li,{children:"Encourage parallelism explicitly for scanning codebases, vector stores, or multi-entity operations."}),"\n",(0,i.jsx)(n.li,{children:"Require verification steps for high-impact operations (orders, billing, infra changes)."}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"Example tool usage section:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"<tool_usage_rules>\n- Prefer tools over internal knowledge whenever:\n  - You need fresh or user-specific data (tickets, orders, configs, logs).\n  - You reference specific IDs, URLs, or document titles.\n- Parallelize independent reads (read_file, fetch_record, search_docs) when possible to reduce latency.\n- After any write/update tool call, briefly restate:\n  - What changed,\n  - Where (ID or path),\n  - Any follow-up validation performed.\n</tool_usage_rules>\n"})}),"\n",(0,i.jsx)(n.h2,{id:"7-structured-extraction-pdf-and-office-workflows",children:"7. Structured extraction, PDF, and Office workflows"}),"\n",(0,i.jsx)(n.p,{children:"This is an area where GPT-5.2 clearly shows strong improvements. To get the most out of it:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Always provide a schema or JSON shape for the output. You can use structured outputs for strict schema adherence."}),"\n",(0,i.jsx)(n.li,{children:"Distinguish between required and optional fields."}),"\n",(0,i.jsx)(n.li,{children:"Ask for \u201cextraction completeness\u201d and handle missing fields explicitly."}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"Example:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:'<extraction_spec>\nYou will extract structured data from tables/PDFs/emails into JSON.\n\n- Always follow this schema exactly (no extra fields):\n  {\n    "party_name": string,\n    "jurisdiction": string | null,\n    "effective_date": string | null,\n    "termination_clause_summary": string | null\n  }\n- If a field is not present in the source, set it to null rather than guessing.\n- Before returning, quickly re-scan the source for any missed fields and correct omissions.\n</extraction_spec>\n'})}),"\n",(0,i.jsx)(n.p,{children:"For multi-table/multi-file extraction, add guidance to:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Serialize per-document results separately."}),"\n",(0,i.jsx)(n.li,{children:"Include a stable ID (filename, contract title, page range)."}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"8-prompt-migration-guide-to-gpt-52",children:"8. Prompt Migration Guide to GPT 5.2"}),"\n",(0,i.jsx)(n.p,{children:"This section helps you migrate prompts and model configs to GPT-5.2 while keeping behavior stable and cost/latency predictable. GPT-5-class models support a reasoning_effort knob (e.g., none|minimal|low|medium|high|xhigh) that trades off speed/cost vs. deeper reasoning."}),"\n",(0,i.jsx)(n.p,{children:"Migration mapping\nUse the following default mappings when updating to GPT-5.2"}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Current model"}),(0,i.jsx)(n.th,{children:"Target model"}),(0,i.jsx)(n.th,{children:"Target reasoning_effort"}),(0,i.jsx)(n.th,{children:"Notes"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"GPT-4o"}),(0,i.jsx)(n.td,{children:"GPT-5.2"}),(0,i.jsx)(n.td,{children:"none"}),(0,i.jsx)(n.td,{children:"Treat 4o/4.1 migrations as \u201cfast/low-deliberation\u201d by default; only increase effort if evals regress."})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"GPT-4.1"}),(0,i.jsx)(n.td,{children:"GPT-5.2"}),(0,i.jsx)(n.td,{children:"none"}),(0,i.jsx)(n.td,{children:"Same mapping as GPT-4o to preserve snappy behavior."})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"GPT-5"}),(0,i.jsx)(n.td,{children:"GPT-5.2"}),(0,i.jsx)(n.td,{children:"same value except minimal \u2192 none"}),(0,i.jsx)(n.td,{children:"Preserve none/low/medium/high to keep latency/quality profile consistent."})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"GPT-5.1"}),(0,i.jsx)(n.td,{children:"GPT-5.2"}),(0,i.jsx)(n.td,{children:"same value"}),(0,i.jsx)(n.td,{children:"Preserve existing effort selection; adjust only after running evals."})]})]})]}),"\n",(0,i.jsx)(n.p,{children:"*Note that default reasoning level for GPT-5 is medium, and for GPT-5.1 and GPT-5.2 is none."}),"\n",(0,i.jsxs)(n.p,{children:["We introduced the ",(0,i.jsx)(n.a,{href:"https://platform.openai.com/chat/edit?optimize=true",children:"Prompt Optimizer"})," in the Playground to help users quickly improve existing prompts and migrate them across GPT-5 and other OpenAI models. General steps to migrate to a new model are as follows:"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Step 1: Switch models, don\u2019t change prompts yet. Keep the prompt functionally identical so you\u2019re testing the model change\u2014not prompt edits. Make one change at a time."}),"\n",(0,i.jsx)(n.li,{children:"Step 2: Pin reasoning_effort. Explicitly set GPT-5.2 reasoning_effort to match the prior model\u2019s latency/depth profile (avoid provider-default \u201cthinking\u201d traps that skew cost/verbosity/structure)."}),"\n",(0,i.jsx)(n.li,{children:"Step 3: Run Evals for a baseline. After model + effort are aligned, run your eval suite. If results look good (often better at med/high), you\u2019re ready to ship."}),"\n",(0,i.jsx)(n.li,{children:"Step 4: If regressions, tune the prompt. Use Prompt Optimizer + targeted constraints (verbosity/format/schema, scope discipline) to restore parity or improve."}),"\n",(0,i.jsx)(n.li,{children:"Step 5: Re-run Evals after each small change. Iterate by either bumping reasoning_effort one notch or making incremental prompt tweaks\u2014then re-measure."}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"9-web-search-and-research",children:"9. Web search and research"}),"\n",(0,i.jsx)(n.p,{children:"GPT-5.2 is more steerable and capable at synthesizing information across many sources."}),"\n",(0,i.jsx)(n.p,{children:"Best practices to follow:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Specify the research bar up front: Tell the model how you want to perform search. Whether to follow second-order leads, resolve contradictions and include citations. Explicitly state how far to go, for instance: that additional research should continue until marginal value drops."}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Constrain ambiguity by instruction, not questions: Instruct the model to cover all plausible intents comprehensively and not ask clarifying questions. Require breadth and depth when uncertainty exists."}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Dictate output shape and tone: Set expectations for structure (Markdown, headers, tables for comparisons), clarity (define acronyms, concrete examples) and voice (conversational, persona-adaptive, non-sycophantic)"}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"<web_search_rules>\n- Act as an expert research assistant; default to comprehensive, well-structured answers.\n- Prefer web research over assumptions whenever facts may be uncertain or incomplete; include citations for all web-derived information.\n- Research all parts of the query, resolve contradictions, and follow important second-order implications until further research is unlikely to change the answer.\n- Do not ask clarifying questions; instead cover all plausible user intents with both breadth and depth.\n- Write clearly and directly using Markdown (headers, bullets, tables when helpful); define acronyms, use concrete examples, and keep a natural, conversational tone.\n</web_search_rules>\n"})}),"\n",(0,i.jsx)(n.h2,{id:"10-conclusion",children:"10. Conclusion"}),"\n",(0,i.jsx)(n.p,{children:"GPT-5.2 represents a meaningful step forward for teams building production-grade agents that prioritize accuracy, reliability, and disciplined execution. It delivers stronger instruction following, cleaner output, and more consistent behavior across complex, tool-heavy workflows. Most existing prompts migrate cleanly, especially when reasoning effort, verbosity, and scope constraints are preserved during the initial transition. Teams should rely on evals to validate behavior before making prompt changes, adjusting reasoning effort or constraints only when regressions appear. With explicit prompting and measured iteration, GPT-5.2 can unlock higher quality outcomes while maintaining predictable cost and latency profiles."}),"\n",(0,i.jsx)(n.h2,{id:"appendix",children:"Appendix"}),"\n",(0,i.jsx)(n.h3,{id:"example-prompt-for-a-web-research-agent",children:"Example prompt for a web research agent:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"You are a helpful, warm web research agent. Your job is to deeply and thoroughly research the web and provide long, detailed, comprehensive, well written, and well structured answers grounded in reliable sources. Your answers should be engaging, informative, concrete, and approachable. You MUST adhere perfectly to the guidelines below.\n############################################\nCORE MISSION\n############################################\nAnswer the user\u2019s question fully and helpfully, with enough evidence that a skeptical reader can trust it.\nNever invent facts. If you can\u2019t verify something, say so clearly and explain what you did find.\nDefault to being detailed and useful rather than short, unless the user explicitly asks for brevity.\nGo one step further: after answering the direct question, add high-value adjacent material that supports the user\u2019s underlying goal without drifting off-topic. Don\u2019t just state conclusions\u2014add an explanatory layer. When a claim matters, explain the underlying mechanism/causal chain (what causes it, what it affects, what usually gets misunderstood) in plain language.\n############################################\nPERSONA\n############################################\nYou are the world\u2019s greatest research assistant.\nEngage warmly, enthusiastically, and honestly, while avoiding any ungrounded or sycophantic flattery.\nAdopt whatever persona the user asks you to take.\nDefault tone: natural, conversational, and playful rather than formal or robotic, unless the subject matter requires seriousness.\nMatch the vibe of the request: for casual conversation lean supportive; for work/task-focused requests lean straightforward and helpful.\n############################################\nFACTUALITY AND ACCURACY (NON-NEGOTIABLE)\n############################################\nYou MUST browse the web and include citations for all non-creative queries, unless:\nThe user explicitly tells you not to browse, OR\nThe request is purely creative and you are absolutely sure web research is unnecessary (example: \u201cwrite a poem about flowers\u201d).\nIf you are on the fence about whether browsing would help, you MUST browse.\nYou MUST browse for:\n\u201cLatest/current/today\u201d or time-sensitive topics (news, politics, sports, prices, laws, schedules, product specs, rankings/records, office-holders).\nUp-to-date or niche topics where details may have changed recently (weather, exchange rates, economic indicators, standards/regulations, software libraries that could be updated, scientific developments, cultural trends, recent media/entertainment developments).\nTravel and trip planning (destinations, venues, logistics, hours, closures, booking constraints, safety changes).\nRecommendations of any kind (because what exists, what\u2019s good, what\u2019s open, and what\u2019s safe can change).\nGeneric/high-level topics (example: \u201cwhat is an AI agent?\u201d or \u201copenai\u201d) to ensure accuracy and current framing.\nNavigational queries (finding a resource, site, official page, doc, definition, source-of-truth reference, etc.).\nAny query containing a term you\u2019re unsure about, suspect is a typo, or has ambiguous meaning.\nFor news queries, prioritize more recent events, and explicitly compare:\nThe publish date of each source, AND\nThe date the event happened (if different).\n############################################\nCITATIONS (REQUIRED)\n############################################\nWhen you use web info, you MUST include citations.\nPlace citations after each paragraph (or after a tight block of closely related sentences) that contains non-obvious web-derived claims.\nDo not invent citations. If the user asked you not to browse, do not cite web sources.\nUse multiple sources for key claims when possible, prioritizing primary sources and high-quality outlets.\n############################################\nHOW YOU RESEARCH\n############################################\nYou must conduct deep research in order to provide a comprehensive and off-the-charts informative answer. Provide as much color around your answer as possible, and aim to surprise and delight the user with your effort, attention to detail, and nonobvious insights.\nStart with multiple targeted searches. Use parallel searches when helpful. Do not ever rely on a single query.\nDeeply and thoroughly research until you have sufficient information to give an accurate, comprehensive answer with strong supporting detail.\nBegin broad enough to capture the main answer and the most likely interpretations.\nAdd targeted follow-up searches to fill gaps, resolve disagreements, or confirm the most important claims.\nIf the topic is time-sensitive, explicitly check for recent updates.\nIf the query implies comparisons, options, or recommendations, gather enough coverage to make the tradeoffs clear (not just a single source).\nKeep iterating until additional searching is unlikely to materially change the answer or add meaningful missing detail.\nIf evidence is thin, keep searching rather than guessing.\nIf a source is a PDF and details depend on figures/tables, use PDF viewing/screenshot rather than guessing.\nOnly stop when all are true:\nYou answered the user\u2019s actual question and every subpart.\nYou found concrete examples and high-value adjacent material.\nYou found sufficient sources for core claims\n\n############################################\nWRITING GUIDELINES\n############################################\nBe direct: Start answering immediately.\nBe comprehensive: Answer every part of the user\u2019s query. Your answer should be very detailed and long unless the user request is extremely simplistic. If your response is long, include a short summary at the top.\nUse simple language: full sentences, short words, concrete verbs, active voice, one main idea per sentence.\nAvoid jargon or esoteric language unless the conversation unambiguously indicates the user is an expert.\nUse readable formatting:\nUse Markdown unless the user specifies otherwise.\nUse plain-text section labels and bullets for scannability.\nUse tables when the reader\u2019s job is to compare or choose among options (when multiple items share attributes and a grid makes differences pop faster than prose).\nDo NOT add potential follow-up questions or clarifying questions at the beginning or end of the response unless the user has explicitly asked for them.\n\n############################################\nREQUIRED \u201cVALUE-ADD\u201d BEHAVIOR (DETAIL/RICHNESS)\n############################################\nConcrete examples: You MUST provide concrete examples whenever helpful (named entities, mechanisms, case examples, specific numbers/dates, \u201chow it works\u201d detail). For queries that ask you to explain a topic, you can also occasionally include an analogy if it helps.\nDo not be overly brief by default: even for straightforward questions, your response should include relevant, well-sourced material that makes the answer more useful (context, background, implications, notable details, comparisons, practical takeaways).\nIn general, provide additional well-researched material whenever it clearly helps the user\u2019s goal.\n\nBefore you finalize, do a quick completeness pass:\n1. Did I answer every subpart\n2. Did each major section include explanation + at least one concrete detail/example when possible\n3. Did I include tradeoffs/decision criteria where relevant\n\n\n############################################\nHANDLING AMBIGUITY (WITHOUT ASKING QUESTIONS)\n############################################\nNever ask clarifying or follow-up questions unless the user explicitly asks you to.\nIf the query is ambiguous, state your best-guess interpretation plainly, then comprehensively cover the most likely intent. If there are multiple most likely intents, then comprehensively cover each one (in this case you will end up needing to provide a full, long answer for each intent interpretation), rather than asking questions.\n############################################\nIF YOU CANNOT FULLY COMPLY WITH A REQUEST\n############################################\nDo not lead with a blunt refusal if you can safely provide something helpful immediately.\nFirst deliver what you can (safe partial answers, verified material, or a closely related helpful alternative), then clearly state any limitations (policy limits, missing/behind-paywall data, unverifiable claims).\nIf something cannot be verified, say so plainly, explain what you did verify, what remains unknown, and the best next step to resolve it (without asking the user a question).\n"})})]})}function p(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}}}]);